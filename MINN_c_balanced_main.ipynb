{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINN experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to reproduced the results regarding the MINN presented in the paper.\n",
    "Specifically, \n",
    "- the MINN-unbalanced\n",
    "- MINN-c-balanced \n",
    "-  MINN-c-balanced (FBA reduced gem)\n",
    "-  MINN-c-balance (FBA solution data) \n",
    "- MINN-divided loss\n",
    " \n",
    " You can select which experiment to reproduce by selecting the relative hydra configuration file in the second cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import pandas as pd\n",
    "from src.nn_model.amn_qp import *\n",
    "from src.utils.hpo import hpo\n",
    "from src.utils.import_data import *\n",
    "from src.utils.import_GEM import *\n",
    "from src.utils.training import *\n",
    "from src.utils.plots import *\n",
    "from src.utils.utils import *\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log hydra configuration file to choose which MINN configuration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydra in jupyter notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "config_dir = \"conf\" # Adjust path\n",
    "initialize(config_path=config_dir, job_name=\"notebook_logging\", version_base=\"1.1\")\n",
    "\n",
    "# HERE YOU CAN DECIDE WHICH CONFIGURATION FILE TO LOG\n",
    "# MINN_c_balanced, MINN_unbalanced, MINN_c_balanced_FBA_fit_data, MINN_c_balanced_GEM_FBA_reduced, MINN_divided_loss\n",
    "\n",
    "cfg = compose(config_name=\"MINN_c_balanced\") \n",
    "\n",
    "# Step 3: Log the configuration\n",
    "logger.info(\"Logging configuration:\")\n",
    "logger.info(OmegaConf.to_yaml(cfg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation pipeline: loocv + kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No clearML when using jupyter\n",
    "task= None\n",
    "# reproduciblility\n",
    "seed = cfg.seed\n",
    "fix_random_seed(seed=seed)\n",
    "\n",
    "# load gem and data\n",
    "n_distribution= cfg.gem.n_total_reactions\n",
    "X, y, Vin, fit_model, reference = load_ishii(seed=seed, \n",
    "                                                dataset=cfg.dataset.dataset_name,\n",
    "                                                fluxes_removed_from_reference=cfg.dataset.fluxes_removed_from_reference,\n",
    "                                                fluxes_to_add_input = cfg.dataset.fluxes_to_add_input,\n",
    "                                                kos_genes= cfg.dataset.kos_genes,\n",
    "                                                gem = cfg.gem.gem_name)\n",
    "\n",
    "# Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Lists to store results for each LOO iteration\n",
    "Q2_loo = []\n",
    "MAE_loo = []\n",
    "RMSE_loo = []\n",
    "NE_loo = []\n",
    "\n",
    "# List to store single left out predictions\n",
    "Vref_true_loo = []\n",
    "Vref_pred_loo = []\n",
    "Vpred_final = []\n",
    "list_SV_loss_per_sample = []\n",
    "\n",
    "total_run= 29\n",
    "count = 1\n",
    "for train_idx, val_idx in loo.split(X):\n",
    "    # Extract the training and validation data\n",
    "    X_train, y_train, Vin_train = X[train_idx], y[train_idx], Vin[train_idx]\n",
    "    X_val, y_val, Vin_val = X[val_idx], y[val_idx], Vin[val_idx]\n",
    "\n",
    "    # kfolf cross validation \n",
    "    best_params = hpo(cfg, task=task, X_train=X_train, y_train=y_train, Vin_train=Vin_train, n_distribution=n_distribution, fit_model=fit_model)\n",
    "    \n",
    "    # train and test using the best hyperparameters found with the kfold\n",
    "    results = train_test_evaluation(cfg, task, X_train, X_val, y_train, y_val, Vin_train, Vin_val, n_distribution, fit_model, best_params, loo_count=count)\n",
    "    \n",
    "\n",
    "    # Inverse transform targets to extract metrics\n",
    "    Vref_pred_te = np.matmul(np.array(results[\"test\"]['Vref_pred']), fit_model.Pref.T)\n",
    "    Vref_true = results[\"test\"]['Vref_true']\n",
    "    \n",
    "    \n",
    "    # save metrics in the a list (one for each left out observation)\n",
    "    Q2_value = r2_metric(np.array(Vref_true), Vref_pred_te)\n",
    "    Q2_loo.append(Q2_value)\n",
    "\n",
    "    MAE_value = np.mean(np.abs(Vref_true-Vref_pred_te))\n",
    "    MAE_loo.append(MAE_value)\n",
    "\n",
    "    RMSE_value = np.sqrt(np.mean(np.square(Vref_true-Vref_pred_te)))\n",
    "    RMSE_loo.append(RMSE_value)\n",
    "\n",
    "    NE_value = np.nan_to_num(np.linalg.norm(Vref_true - Vref_pred_te) / np.linalg.norm(Vref_true), posinf=0, neginf=0, nan=0)\n",
    "    NE_loo.append(NE_value)\n",
    "\n",
    "    Vref_true_loo.append(np.array(Vref_true).flatten())\n",
    "    Vref_pred_loo.append(np.array(Vref_pred_te).flatten())\n",
    "    Vpred_final.append(np.array(results[\"test\"]['Vref_pred']).flatten())\n",
    "    \n",
    "\n",
    "    print(f'run {count}/{total_run}')\n",
    "    print(f'Loss Train: {results[\"train\"][\"losses\"]}')\n",
    "    print(f'Loss Test: {results[\"test\"][\"losses\"]}')\n",
    "    print(f'QÂ²:{Q2_value}')\n",
    "    print(f'MAE:{MAE_value}')\n",
    "    print(f'RMSE {RMSE_value}')\n",
    "    print(f'NE:{NE_value}')\n",
    "    \n",
    "    list_SV_loss_per_sample.append(results[\"test\"][\"losses\"][1])\n",
    "    count += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report table - DataFrame with index\n",
    "if cfg.dataset.dataset_name=='ref_47_fluxes':\n",
    "    df_true = pd.read_csv('data/ishii_data/fluxomics_iAF1260_reduced_split.csv')[cfg.dataset.metric_fluxes]\n",
    "    \n",
    "elif cfg.dataset.dataset_name=='ref_47_fluxes_fit':\n",
    "    df_true = pd.read_csv('data/ishii_data/fluxomics_iAF1260_reduced_split_fit.csv')[cfg.dataset.metric_fluxes]\n",
    "    \n",
    "else:\n",
    "    df_true = pd.read_csv('data/ishii_data/fluxomics_ecore_correct.csv')[cfg.dataset.metric_fluxes]\n",
    "\n",
    "\n",
    "# Report table - DataFrame with index\n",
    "df_V = pd.DataFrame(Vpred_final, columns=fit_model.reactions)[cfg.dataset.metric_fluxes]\n",
    "\n",
    "\n",
    "metrics_df = metrics_table(df_true, df_V)\n",
    "#add row with SV avg and std\n",
    "avg_SV = np.array(list_SV_loss_per_sample).mean()\n",
    "std_SV = np.array(list_SV_loss_per_sample).std()\n",
    "metrics_df.loc[\"SV loss\"] = [avg_SV, std_SV]\n",
    "\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
